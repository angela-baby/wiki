== 基本信息 ==
=== 背景 ===
    存在中文分词技术，是由于中文在基本文法上有其特殊性，具体表现在：
    1．与英文为代表的拉丁语系语言相比，英文以空格作为天然的分隔符，而中文由于继
承自古代汉语的传统，词语之间没有分隔。　古代汉语中除了连绵词和人名地名等，词通
常就是单个汉字，所以当时没有分词书写的必要。而现代汉语中双字或多字词居多，一个
字不再等同于一个词。
    2．在中文里，“词”和“词组”边界模糊现代汉语的基本表达单元虽然为“词”，且
以双字或者多字词居多，但由于人们认识水平的不同，对词和短语的边界很难去区分。例
如：“对随地吐痰者给予处罚”，“随地吐痰者”本身是一个词还是一个短语，不同的人
会有不同的标准，同样的“海上”“酒厂”等等，即使是同一个人也可能做出不同判断，
如果汉语真的要分词书写，必然会出现混乱，难度很大。
    中文分词的方法其实不局限于中文应用，也被应用到英文处理，如手写识别，单词之
间的空格就不很清楚，中文分词方法可以帮助判别英文单词的边界。

=== 作用 ===
    中文分词是文本挖掘的基础，对于输入的一段中文，成功的进行中文分词，可以达到
电脑自动识别语句含义的效果。
    中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明
白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。

=== 影响 ===
    中文分词对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中
找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面
，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序
。从定性分析来说，搜索引擎的分词算法不同，词库的不同都会影响页面的返回结果[1] 。

=== 算法分类 ===
    现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和
基于统计的分词方法。按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词
与标注相结合的一体化方法。

1. *字符匹配*
    这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分
大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个
词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长
度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；常用的几种机械分
词方法如下：
    1）正向最大匹配法（由左到右的方向）；
    2）逆向最大匹配法（由右到左的方向）；
    3）最少切分（使每一句中切出的词数最小）；
    4）双向最大匹配法（进行由左到右、由右到左两次扫描）
    将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结
合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很
少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计
结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为
1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词
作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。
    一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和
切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机
械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的
词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，
从而极大地提高切分的准确率。
    对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不
做详细论述。

2. *理解法*
    这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想
就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通
常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词
子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人
对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的
笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的
分词系统还处在试验阶段。

3. *统计法*
    从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，
就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度
。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两
个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系
的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方
法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计
取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常
用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词
的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常
用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配
结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文
识别生词、自动消除歧义的优点。
    另外一类是基于统计机器学习的方法。首先给出大量已经分词的文本，利用统计机器
学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。我们知道，汉
语中各个字单独作词语的能力是不同的，此外有的字常常作为前缀出现，有的字却常常作
为后缀（“者”“性”），结合两个字相临时是否成词的信息，这样就得到了许多与分词
有关的知识。这种方法就是充分利用汉语组词的规律来分词。这种方法的最大缺点是需要
有大量预先分好词的语料作支撑，而且训练过程中时空开销极大。
    到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说
，不可能单独依靠某一种算法来实现，都需要综合不同的算法。例如，海量科技的分词算
法就采用“复方分词法”，所谓复方，就是像中西医结合般综合运用机械方法和知识方法
。对于成熟的中文分词系统，需要多种算法综合处理问题。

=== 技术难点 ===
    有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文
是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难
题一直没有完全突破。
* *歧义识别*
    歧义是指同样的一句话，可能有两种或者更多的切分方法。主要的歧义有两种：交集
型歧义和组合型歧义，例如：表面的，因为“表面”和“面的”都是词，那么这个短语就
可以分成“表面的”和“表面的”。这种称为交集型歧义（交叉歧义）。像这种交集型歧
义十分常见，前面举的“和服”的例子，其实就是因为交集型歧义引起的错误。“化妆和
服装”可以分成“化妆和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算
机很难知道到底哪个方案正确。

    交集型歧义相对组合型歧义来说是还算比较容易处理，组合型歧义就必须根据整个句
子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手
拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个
词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何
去识别?

    如果交集型歧义和组合型歧义计算机都能解决的话，在歧义中还有一个难题，是真歧
义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例
如：“乒乓球拍卖完了”，可以切分成“乒乓球拍 卖 完 了”、也可切分成“乒乓球 
拍卖 完了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个
词。

* *新词识别*
    命名实体（人名、地名）、新词，专业术语称为未登录词。也就是那些在分词词典中
没有收录，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解。句子“王
军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就
困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时
每刻都有新增的人名，收录这些人名本身就是一项既不划算又巨大的工程。即使这项工作
可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能
算词？ 除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处
理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的
新词识别十分重要。新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。

== 总结 ==
在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的
处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信
息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合
成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会
影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国
市场，首先也是要解决中文分词问题。

分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索
引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过
长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度
，二者都需要达到很高的要求。研究中文分词的大多是科研院校，清华、北大、哈工大、
中科院、北京语言大学、山西大学、东北大学、IBM研究院、微软中国研究院等都有自己
的研究队伍。
